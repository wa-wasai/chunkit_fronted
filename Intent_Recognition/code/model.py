import os
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.preprocessing import StandardScaler
import numpy as np
from typing import Tuple, Dict, Any, Optional
from collections import Counter
import warnings

warnings.filterwarnings('ignore')


class IntentRecognitionModel:
    """
    æ„å›¾è¯†åˆ«æ¨¡å‹ï¼Œå°è£…äº†éšæœºæ£®æ—åˆ†ç±»å™¨çš„è®­ç»ƒã€é¢„æµ‹ã€ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½ã€‚
    æ”¯æŒè¶…å‚æ•°ä¼˜åŒ–å’Œå…¨é¢çš„æ¨¡å‹è¯„ä¼°ã€‚
    """

    def __init__(self,
                 n_estimators: int = 50,#æ£®æ—ä¸­çš„æ•°é‡ã€‚
                 max_depth: Optional[int] = 30,#æ ‘çš„æœ€å¤§æ·±åº¦
                 min_samples_split: int = 15,  # åˆ†å‰²å†…éƒ¨èŠ‚ç‚¹æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º5ã€‚è¯¥å‚æ•°ç”¨äºæ§åˆ¶è¿‡æ‹Ÿåˆï¼Œè¾ƒå¤§çš„å€¼å¯ä»¥é˜²æ­¢æ¨¡å‹å­¦ä¹ è¿‡äºå…·ä½“çš„è§„åˆ™ï¼Œä»è€Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚
                 min_samples_leaf: int = 4,   # å¶èŠ‚ç‚¹ä¸Šçš„æœ€å°æ ·æœ¬æ•°ï¼Œé»˜è®¤ä¸º2ã€‚æ­¤å‚æ•°ç¡®ä¿å¶å­èŠ‚ç‚¹ä¸ä¼šå¤ªå°ï¼Œæœ‰åŠ©äºå‡å°‘æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„è¿‡åº¦æ‹Ÿåˆï¼Œæå‡æ¨¡å‹çš„ç¨³å®šæ€§ã€‚
                 max_features: str = 'sqrt',#æ¯æ¬¡åˆ†è£‚æ—¶è€ƒè™‘çš„ç‰¹å¾æ•°é‡ã€‚'sqrt'è¡¨ç¤ºå–æ€»ç‰¹å¾æ•°çš„å¹³æ–¹æ ¹ï¼Œèƒ½é™ä½ç›¸å…³æ€§ï¼›è¿™é‡Œå…¶å®ä¹Ÿå¯ä»¥æŒ‡å®šæ¯”ä¾‹æ¯”å¦‚'log2'
                 bootstrap: bool = True,#æ˜¯å¦åœ¨æ„é€ æ ‘çš„æ—¶å€™ä½¿ç”¨æœ‰æ”¾å›æŠ½æ ·ï¼ŒTrueè¡¨è¿°å…¸å‹çš„éšæœºæ£®æ—
                 oob_score: bool = True,#æ˜¯å¦ä½¿ç”¨è¢‹å¤–æ ·æœ¬ä¼°è®¡æ³›åŒ–è¯¯å·®ï¼ˆOut-Of-Bagè¯„åˆ†ï¼‰
                 n_jobs: int = -1,# -1è¡¨è¿°ä½¿ç”¨å…¨éƒ¨å¯ç”¨æ ¸å¿ƒ
                 random_state: int = 42,#éšæœºç§å­
                 class_weight: str = 'balanced'):#ç±»åˆ«æƒé‡ç­–ç•¥ã€‚è®¾ç½®ä¸º'balanced'æ—¶ï¼Œä¼šæ ¹æ®æ ·æœ¬åˆ†å¸ƒå­—å…¸è·³è½¬å„ç±»çš„æƒé‡ï¼Œç¼“è§£ç±»åˆ«ä¸å¹³è¡¡çš„é—®é¢˜
        """
        å°äº5çš„ä¸ä¼šè¢«åˆ†å‰²æˆä¸€ä¸ªå†…éƒ¨èŠ‚ç‚¹ï¼Œå°äº2çš„ä¸ä¼šè¢«åˆ†å‰²æˆä¸€ä¸ªå¶èŠ‚ç‚¹

        """
        """
        åˆå§‹åŒ–æ¨¡å‹ã€‚

        Args:
            n_estimators (int): éšæœºæ£®æ—ä¸­æ ‘çš„æ•°é‡ï¼Œå»ºè®®100-300
            max_depth (Optional[int]): æ ‘çš„æœ€å¤§æ·±åº¦ï¼ŒNoneè¡¨ç¤ºä¸é™åˆ¶
            min_samples_split (int): å†…éƒ¨èŠ‚ç‚¹åˆ†å‰²æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
            min_samples_leaf (int): å¶å­èŠ‚ç‚¹çš„æœ€å°æ ·æœ¬æ•°
            max_features (str): æ¯æ¬¡åˆ†å‰²è€ƒè™‘çš„ç‰¹å¾æ•°é‡ ('sqrt', 'log2', None)
            bootstrap (bool): æ˜¯å¦ä½¿ç”¨bootstrapé‡‡æ ·
            oob_score (bool): æ˜¯å¦è®¡ç®—è¢‹å¤–è¯¯å·®
            n_jobs (int): å¹¶è¡Œä½œä¸šæ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¤„ç†å™¨
            random_state (int): éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°
            class_weight (str): ç±»åˆ«æƒé‡ç­–ç•¥ï¼Œ'balanced'è‡ªåŠ¨å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        """
        self.clf = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf,
            max_features=max_features,
            bootstrap=bootstrap,
            oob_score=oob_score,
            n_jobs=n_jobs,
            random_state=random_state,
            class_weight=class_weight
        )
        
        # åˆå§‹åŒ–æ ‡å‡†åŒ–å™¨ï¼Œç”¨äºç‰¹å¾ç¼©æ”¾
        self.scaler = StandardScaler()
        self.is_trained = False#æ¨¡å‹æ˜¯å¦å·²è®­ç»ƒ
        self.accuracy = None#æ¨¡å‹å‡†ç¡®åº¦
        self.best_params = None#æœ€ä½³å‚æ•°å­—å…¸
        self.class_distribution = None#ç±»åˆ«åˆ†å¸ƒå­—å…¸
        self.feature_importance = None#ç‰¹å¾é‡è¦æ€§å­—å…¸
        self.cross_val_scores = None#äº¤å‰éªŒè¯å¾—åˆ†

    def analyze_data_distribution(self, y) -> Dict[str, int]:
        """
        åˆ†ææ•°æ®åˆ†å¸ƒï¼Œæ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡

        Args:
            y: æ ‡ç­¾æ•°æ®

        Returns:
            Dict[str, int]: ç±»åˆ«åˆ†å¸ƒå­—å…¸
        """
        class_counts = Counter(y)#ç”¨counteræ¥ç»Ÿè®¡å„æ„å›¾ç±»åˆ«çš„æ ·æœ¬æ•°
        self.class_distribution = dict(class_counts)#è¾“å‡ºä¿å­˜è‡³self.class_distribution

        print("=" * 50)
        print("æ•°æ®åˆ†å¸ƒåˆ†æ")
        print("=" * 50)
        print(f"æ€»æ ·æœ¬æ•°: {len(y)}")
        print(f"ç±»åˆ«æ•°é‡: {len(class_counts)}")
        print("\nå„ç±»åˆ«åˆ†å¸ƒ:")

        for intent, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / len(y)) * 100
            print(f"  {intent}: {count} ({percentage:.1f}%)")

        # æ£€æŸ¥ç±»åˆ«ä¸å¹³è¡¡
        max_count = max(class_counts.values())
        min_count = min(class_counts.values())
        imbalance_ratio = max_count / min_count

        if imbalance_ratio > 3:
            print(f"\nâš ï¸  è­¦å‘Š: æ•°æ®å­˜åœ¨ç±»åˆ«ä¸å¹³è¡¡")
            print(f"æœ€å¤§ç±»åˆ«æ˜¯æœ€å°ç±»åˆ«çš„ {imbalance_ratio:.1f} å€")
            print("å»ºè®®: å·²å¯ç”¨ class_weight='balanced' æ¥å¤„ç†è¿™ä¸ªé—®é¢˜")

        return self.class_distribution

    def optimize_hyperparameters(self, X, y, cv: int = 5, n_iter: int = 50) -> Dict[str, Any]:
        """
        ä½¿ç”¨éšæœºæœç´¢ä¼˜åŒ–è¶…å‚æ•°

        Args:
            X: ç‰¹å¾æ•°æ®
            y: æ ‡ç­¾æ•°æ®
            cv (int): äº¤å‰éªŒè¯æŠ˜æ•°
            n_iter (int): éšæœºæœç´¢è¿­ä»£æ¬¡æ•°

        Returns:
            Dict[str, Any]: æœ€ä½³å‚æ•°å­—å…¸
        """
        print("\n" + "=" * 50)
        print("å¼€å§‹è¶…å‚æ•°ä¼˜åŒ–")
        print("=" * 50)

        # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
        param_distributions = {
            'n_estimators': [50, 100, 150, 200, 300],
            'max_depth': [None, 10, 20, 30, 40],
            'min_samples_split': [2, 5, 10, 15],
            'min_samples_leaf': [1, 2, 4, 8],
            'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
            'bootstrap': [True, False]
        }

        # åˆ›å»ºåŸºç¡€åˆ†ç±»å™¨
        base_clf = RandomForestClassifier(
            oob_score=True,
            n_jobs=-1,
            random_state=42,
            class_weight='balanced'
        )

        # éšæœºæœç´¢
        print(f"æ­£åœ¨å°è¯• {n_iter} ç§å‚æ•°ç»„åˆ...")
        random_search = RandomizedSearchCV(
            estimator=base_clf,
            param_distributions=param_distributions,
            n_iter=n_iter,
            cv=cv,
            scoring='accuracy',
            n_jobs=-1,
            random_state=42,
            verbose=0  # å‡å°‘è¾“å‡º
        )

        # æ‰§è¡Œæœç´¢
        random_search.fit(X, y)

        # ä¿å­˜æœ€ä½³å‚æ•°
        self.best_params = random_search.best_params_
        self.clf = random_search.best_estimator_

        print(f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ!")
        print(f"æœ€ä½³äº¤å‰éªŒè¯å¾—åˆ†: {random_search.best_score_:.4f}")
        print("æœ€ä½³å‚æ•°:")
        for param, value in self.best_params.items():
            print(f"  {param}: {value}")

        return self.best_params

    def train(self, X, y, test_size: float = 0.20, optimize: bool = False, validate: bool = True):
        """
        è®­ç»ƒæ¨¡å‹ï¼Œæ”¯æŒè¶…å‚æ•°ä¼˜åŒ–å’Œå…¨é¢è¯„ä¼°

        Args:
            X: ç‰¹å¾æ•°æ®ï¼ˆæ–‡æœ¬å‘é‡ï¼‰
            y: æ ‡ç­¾æ•°æ®ï¼ˆæ„å›¾ï¼‰
            test_size (float): æµ‹è¯•é›†æ¯”ä¾‹
            optimize (bool): æ˜¯å¦è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
            validate (bool): æ˜¯å¦è¿›è¡Œè¯¦ç»†éªŒè¯è¯„ä¼°
        """
        print("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        # åˆ†ææ•°æ®åˆ†å¸ƒ
        self.analyze_data_distribution(y)

        # æ•°æ®åˆ†å‰²ï¼ˆä½¿ç”¨åˆ†å±‚é‡‡æ ·ä¿æŒç±»åˆ«æ¯”ä¾‹ï¼‰
        print(f"\nåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›† (æµ‹è¯•é›†æ¯”ä¾‹: {test_size})...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y,
            test_size=test_size,
            random_state=42,
            stratify=y
        )

        print(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

        # ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆè™½ç„¶éšæœºæ£®æ—å¯¹æ­¤ä¸æ•æ„Ÿï¼Œä½†å¯èƒ½æœ‰å¸®åŠ©ï¼‰
        print("\nè¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–...")
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        # è¶…å‚æ•°ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
        if optimize:
            print("\nğŸ”§ å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–...")
            self.optimize_hyperparameters(X_train_scaled, y_train)
        else:
            print("\nä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ...")

        # è®­ç»ƒæ¨¡å‹
        print("\nè®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        self.clf.fit(X_train_scaled, y_train)
        self.is_trained = True

        # æ¨¡å‹éªŒè¯
        if validate:
            self._comprehensive_evaluation(X_train_scaled, X_test_scaled, y_train, y_test)

        print("\nâœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    def _comprehensive_evaluation(self, X_train, X_test, y_train, y_test):
        """
        å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
        """
        print("\n" + "=" * 50)
        print("æ¨¡å‹æ€§èƒ½è¯„ä¼°")
        print("=" * 50)

        # åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
        train_accuracy = self.clf.score(X_train, y_train)
        test_accuracy = self.clf.score(X_test, y_test)
        self.accuracy = test_accuracy

        print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_accuracy:.4f}")
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_accuracy:.4f}")

        # è¿‡æ‹Ÿåˆæ£€æŸ¥
        overfitting = train_accuracy - test_accuracy
        if overfitting > 0.1:
            print(f"âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (å·®å¼‚: {overfitting:.4f})")
            print("å»ºè®®: å¢åŠ  min_samples_leaf æˆ–å‡å°‘ max_depth")
        elif overfitting < 0.02:
            print("âœ… æ¨¡å‹æ‹Ÿåˆè‰¯å¥½")

        # è¢‹å¤–è¯¯å·®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self.clf, 'oob_score_') and self.clf.oob_score_:
            print(f"è¢‹å¤–å‡†ç¡®ç‡: {self.clf.oob_score_:.4f}")

        # äº¤å‰éªŒè¯
        print("\nè¿›è¡Œ5æŠ˜äº¤å‰éªŒè¯...")
        cv_scores = cross_val_score(self.clf, X_train, y_train, cv=5, scoring='accuracy')
        self.cross_val_scores = cv_scores
        print(f"äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (Â±{cv_scores.std() * 2:.4f})")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        y_pred = self.clf.predict(X_test)
        print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(y_test, y_pred, digits=4))

        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        self._analyze_feature_importance()

        # æ··æ·†çŸ©é˜µ
        if len(set(y_test)) <= 10:  # åªä¸ºè¾ƒå°‘ç±»åˆ«æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_test, y_pred)
            print(f"\næ··æ·†çŸ©é˜µ:")
            print(cm)

    def _analyze_feature_importance(self, top_n: int = 20):
        """
        åˆ†æå’Œæ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
        """
        if hasattr(self.clf, 'feature_importances_'):
            self.feature_importance = self.clf.feature_importances_

            # ç»Ÿè®¡ä¿¡æ¯
            total_features = len(self.feature_importance)
            non_zero_features = np.sum(self.feature_importance > 0)
            avg_importance = np.mean(self.feature_importance)

            print(f"\nç‰¹å¾é‡è¦æ€§åˆ†æ:")
            print(f"æ€»ç‰¹å¾æ•°: {total_features}")
            print(f"æœ‰è´¡çŒ®ç‰¹å¾æ•°: {non_zero_features}")
            print(f"å¹³å‡é‡è¦æ€§: {avg_importance:.6f}")

            # æ˜¾ç¤ºæœ€é‡è¦çš„ç‰¹å¾
            indices = np.argsort(self.feature_importance)[::-1]
            print(f"\nå‰{min(top_n, total_features)}ä¸ªæœ€é‡è¦ç‰¹å¾:")
            for i in range(min(top_n, total_features)):
                importance = self.feature_importance[indices[i]]
                if importance > 0:
                    print(f"  ç‰¹å¾ {indices[i]:4d}: {importance:.6f}")

    def predict(self, vector: np.ndarray) -> Tuple[str, float, np.ndarray, np.ndarray]:
        """
        å¯¹å•ä¸ªå‘é‡è¿›è¡Œæ„å›¾é¢„æµ‹

        Args:
            vector (np.ndarray): å•ä¸ªæ–‡æœ¬çš„å‘é‡è¡¨ç¤º

        Returns:
            Tuple[str, float, np.ndarray, np.ndarray]:
                é¢„æµ‹æ„å›¾ã€ç½®ä¿¡åº¦ã€æ‰€æœ‰ç±»åˆ«æ¦‚ç‡ã€ç±»åˆ«æ ‡ç­¾
        """
        if not self.is_trained:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨ train() æˆ– load_model() æ–¹æ³•ã€‚")

        # ç¡®ä¿è¾“å…¥æ˜¯äºŒç»´æ•°ç»„
        if vector.ndim == 1:
            vector = vector.reshape(1, -1)

        # ç‰¹å¾æ ‡å‡†åŒ–
        vector_scaled = self.scaler.transform(vector)

        # é¢„æµ‹æ¦‚ç‡
        probabilities = self.clf.predict_proba(vector_scaled)[0]
        max_prob_index = np.argmax(probabilities)
        max_prob = probabilities[max_prob_index]
        predicted_intent = self.clf.classes_[max_prob_index]

        return predicted_intent, max_prob, probabilities, self.clf.classes_

    def predict_batch(self, vectors: np.ndarray, confidence_threshold: float = 0.5) -> list:
        """
        æ‰¹é‡é¢„æµ‹ï¼Œæ”¯æŒç½®ä¿¡åº¦é˜ˆå€¼

        Args:
            vectors (np.ndarray): æ–‡æœ¬å‘é‡çŸ©é˜µ
            confidence_threshold (float): ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„é¢„æµ‹ä¼šè¢«æ ‡è®°

        Returns:
            list: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«é¢„æµ‹ä¿¡æ¯
        """
        if not self.is_trained:
            raise RuntimeError("æ¨¡å‹å°šæœªè®­ç»ƒã€‚è¯·å…ˆè°ƒç”¨ train() æˆ– load_model() æ–¹æ³•ã€‚")

        # ç‰¹å¾æ ‡å‡†åŒ–
        vectors_scaled = self.scaler.transform(vectors)

        # æ‰¹é‡é¢„æµ‹
        probabilities = self.clf.predict_proba(vectors_scaled)
        predicted_classes = self.clf.predict(vectors_scaled)

        results = []
        for i, (pred_class, probs) in enumerate(zip(predicted_classes, probabilities)):
            max_prob = np.max(probs)

            # æ„å»ºç»“æœå­—å…¸
            result = {
                'predicted_intent': pred_class,
                'confidence': float(max_prob),
                'low_confidence': max_prob < confidence_threshold,
                'all_probabilities': {
                    str(cls): float(prob)
                    for cls, prob in zip(self.clf.classes_, probs)
                }
            }
            results.append(result)

        return results

    def save_model(self, path: str):
        """
        ä¿å­˜å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯åˆ°æ–‡ä»¶

        Args:
            path (str): æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        if not self.is_trained:
            raise RuntimeError("æ— æ³•ä¿å­˜æœªè®­ç»ƒçš„æ¨¡å‹ã€‚")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(path), exist_ok=True)

        print(f"ä¿å­˜æ¨¡å‹åˆ° {path}...")

        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ•°æ®
        model_data = {
            "model": self.clf,
            "scaler": self.scaler,
            "accuracy": self.accuracy,
            "best_params": self.best_params,
            "class_distribution": self.class_distribution,
            "feature_importance": self.feature_importance,
            "cross_val_scores": self.cross_val_scores,
            "model_params": self.clf.get_params(),
            "version": "2.0",
            "n_features": self.scaler.n_features_in_ if hasattr(self.scaler, 'n_features_in_') else None
        }

        joblib.dump(model_data, path)
        print("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸï¼")

    def load_model(self, path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

        Args:
            path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„

        Returns:
            bool: åŠ è½½æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not os.path.exists(path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return False

        print(f"ä» {path} åŠ è½½æ¨¡å‹...")

        try:
            data = joblib.load(path)

            if isinstance(data, dict):
                # æ–°ç‰ˆæœ¬æ ¼å¼ - å®Œæ•´æ•°æ®
                self.clf = data["model"]
                self.scaler = data.get("scaler", StandardScaler())
                self.accuracy = data.get("accuracy", None)
                self.best_params = data.get("best_params", None)
                self.class_distribution = data.get("class_distribution", None)
                self.feature_importance = data.get("feature_importance", None)
                self.cross_val_scores = data.get("cross_val_scores", None)

                # æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯
                version = data.get("version", "1.0")
                print(f"æ¨¡å‹ç‰ˆæœ¬: {version}")

            else:
                # æ—§ç‰ˆæœ¬æ ¼å¼ - ä»…æ¨¡å‹
                print("âš ï¸  æ£€æµ‹åˆ°æ—§ç‰ˆæœ¬æ¨¡å‹æ ¼å¼")
                self.clf = data
                self.scaler = StandardScaler()
                print("è­¦å‘Š: ç¼ºå°‘é¢„å¤„ç†å™¨ï¼Œå¯èƒ½å½±å“é¢„æµ‹å‡†ç¡®æ€§")

            # éªŒè¯æ¨¡å‹æœ‰æ•ˆæ€§
            if hasattr(self.clf, 'predict_proba') and hasattr(self.clf, 'classes_'):
                self.is_trained = True

                # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
                accuracy_info = f"å‡†ç¡®ç‡: {self.accuracy:.4f}" if self.accuracy else "å‡†ç¡®ç‡æœªçŸ¥"
                print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ {accuracy_info}")

                if self.class_distribution:
                    print(f"æ”¯æŒ {len(self.class_distribution)} ä¸ªæ„å›¾ç±»åˆ«")

                if hasattr(self.clf, 'n_estimators'):
                    print(f"éšæœºæ£®æ—æ ‘æ•°é‡: {self.clf.n_estimators}")

                return True
            else:
                print(f"âŒ åŠ è½½çš„å¯¹è±¡ä¸æ˜¯æœ‰æ•ˆçš„åˆ†ç±»å™¨")
                return False

        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
            return False

    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯

        Returns:
            Dict[str, Any]: åŒ…å«æ¨¡å‹å„ç§ä¿¡æ¯çš„å­—å…¸
        """
        if not self.is_trained:
            return {"status": "æœªè®­ç»ƒ"}

        info = {
            "status": "å·²è®­ç»ƒ",
            "accuracy": self.accuracy,
            "model_type": "RandomForestClassifier",
            "model_params": self.clf.get_params(),
            "best_params": self.best_params,
            "class_distribution": self.class_distribution,
            "n_classes": len(self.clf.classes_),
            "classes": list(self.clf.classes_),
            "n_features": getattr(self.scaler, 'n_features_in_', None)
        }

        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        if hasattr(self.clf, 'oob_score_'):
            info["oob_score"] = self.clf.oob_score_

        if self.cross_val_scores is not None:
            info["cross_val_mean"] = float(np.mean(self.cross_val_scores))
            info["cross_val_std"] = float(np.std(self.cross_val_scores))

        # æ·»åŠ ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
        if self.feature_importance is not None:
            info["feature_importance_stats"] = {
                "mean": float(np.mean(self.feature_importance)),
                "std": float(np.std(self.feature_importance)),
                "max": float(np.max(self.feature_importance)),
                "non_zero_features": int(np.sum(self.feature_importance > 0))
            }

        return info

    def print_model_summary(self):
        """æ‰“å°æ¨¡å‹æ‘˜è¦ä¿¡æ¯"""
        if not self.is_trained:
            print("æ¨¡å‹å°šæœªè®­ç»ƒ")
            return

        info = self.get_model_info()

        print("\n" + "=" * 50)
        print("æ¨¡å‹æ‘˜è¦")
        print("=" * 50)
        print(f"æ¨¡å‹ç±»å‹: {info['model_type']}")
        print(f"å‡†ç¡®ç‡: {info['accuracy']:.4f}")
        print(f"æ”¯æŒç±»åˆ«æ•°: {info['n_classes']}")
        print(f"ç‰¹å¾ç»´åº¦: {info['n_features']}")

        if 'oob_score' in info:
            print(f"è¢‹å¤–å¾—åˆ†: {info['oob_score']:.4f}")

        if 'cross_val_mean' in info:
            print(f"äº¤å‰éªŒè¯: {info['cross_val_mean']:.4f} (Â±{info['cross_val_std']:.4f})")

        print(f"éšæœºæ£®æ—å‚æ•°:")
        key_params = ['n_estimators', 'max_depth', 'min_samples_split', 'min_samples_leaf']
        for param in key_params:
            if param in info['model_params']:
                print(f"  {param}: {info['model_params'][param]}")